{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport os\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tqdm import tqdm\nfrom datasets import load_dataset, Dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## AG News","metadata":{}},{"cell_type":"code","source":"LABEL_TEXTS = [\"world\", \"sports\", \"business\", \"sci/tech\"]\n\nraw = load_dataset(\"ag_news\")\n\ndef add_label_text(batch):\n    batch[\"label_text\"] = LABEL_TEXTS[batch[\"label\"]]\n    return batch\n\nraw = raw.map(add_label_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nMAX_INPUT = 256\nMAX_TARGET = 256\nPREFIX = \"classify: \"\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\ndef tokenize(batch):\n    inputs = [PREFIX + x for x in batch[\"text\"]]\n    targets = batch[\"label_text\"]\n\n    model_inputs = tokenizer(\n        inputs,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_INPUT,\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_TARGET,\n        )\n\n    labels_ids = []\n    for seq in labels[\"input_ids\"]:\n        labels_ids.append([tok if tok != tokenizer.pad_token_id else -100 for tok in seq])\n\n    model_inputs[\"labels\"] = labels_ids\n    return model_inputs\n\ntok = raw.map(tokenize, batched=True)\ntok = tok.remove_columns([\"text\", \"label_text\"])\ntok.set_format(type=\"torch\")\n\ntrain_dataset = tok[\"train\"].select(range(5000))\nval_dataset = tok[\"test\"].select(range(1000))\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\nnum_epochs = 10\n\nbest_val_loss = float(\"inf\")\npatience = 1\npatience_counter = 0\n\nfor epoch in range(num_epochs):\n\n    # -----------------------\n    # TRAINING\n    # -----------------------\n    model.train()\n    total_train_loss = 0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch} training\"):\n        input_ids = batch[\"input_ids\"].to(model.device)\n        attn = batch[\"attention_mask\"].to(model.device)\n        labels = batch[\"labels\"].to(model.device)\n\n        out = model(input_ids, attention_mask=attn, labels=labels)\n        loss = out.loss\n\n        total_train_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n\n    # -----------------------\n    # VALIDATION\n    # -----------------------\n    model.eval()\n    total_val_loss = 0\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Epoch {epoch} validation\"):\n            input_ids = batch[\"input_ids\"].to(model.device)\n            attn = batch[\"attention_mask\"].to(model.device)\n            labels = batch[\"labels\"].to(model.device)\n\n            out = model(input_ids, attention_mask=attn, labels=labels)\n            total_val_loss += out.loss.item()\n\n    avg_val_loss = total_val_loss / len(val_loader)\n\n    print(f\"\\nEpoch {epoch}: Train={avg_train_loss:.4f}  Val={avg_val_loss:.4f}\")\n\n    # -----------------------\n    # EARLY STOPPING\n    # -----------------------\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"agnews-best.pth\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"Training complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def map_prediction_to_label(text):\n    t = text.strip().lower()\n    for lbl in LABEL_TEXTS:\n        if lbl in t:\n            return lbl\n    first = t.split()[0] if t else \"\"\n    for lbl in LABEL_TEXTS:\n        if lbl.startswith(first) or first.startswith(lbl[:3]):\n            return lbl\n    return None\n\nmodel.eval()\ncorrect = 0\ntotal = 0\n\neval_loader = DataLoader(val_dataset, batch_size=4)\n\nwith torch.no_grad():\n    for batch in tqdm(eval_loader, desc=\"Evaluating generation\"):\n        input_ids = batch[\"input_ids\"].to(model.device)\n        attn = batch[\"attention_mask\"].to(model.device)\n\n        gen = model.generate(\n            input_ids=input_ids,\n            attention_mask=attn,\n            max_new_tokens=16,\n            num_beams=4,\n        )\n\n        outputs = tokenizer.batch_decode(gen, skip_special_tokens=True)\n        preds = [map_prediction_to_label(x) for x in outputs]\n        golds = [LABEL_TEXTS[l] for l in batch[\"label\"]]\n\n        for p, g in zip(preds, golds):\n            if p == g:\n                correct += 1\n        total += len(golds)\n\nprint(f\"Generation Accuracy: {100*correct/total:.2f}%  ({correct}/{total})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TinyStories","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"roneneldan/TinyStories\", split=\"train[:5000]\")\n\ndef tokenize_fn(batch):\n    return tokenizer(\n        batch[\"text\"],\n        truncation=True,\n        max_length=256,\n    )\n\ntokenized = ds.map(tokenize_fn, batched=True)\n\ntrain_val = tokenized.train_test_split(test_size=0.1, seed=42)\ntrain_ds = train_val[\"train\"]\nval_ds = train_val[\"test\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef collate_fn(batch):\n    tensors = [\n        torch.tensor(example[\"input_ids\"][:256], dtype=torch.long)\n        for example in batch\n    ]\n    return {\n        \"input_ids\": torch.nn.utils.rnn.pad_sequence(\n            tensors,\n            batch_first=True,\n            padding_value=tokenizer.pad_token_id\n        )\n    }\n\n\ntrain_loader = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate_fn)\nval_loader   = DataLoader(val_ds,   batch_size=2, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\nnum_epochs = 5\n\nbest_val_loss = float(\"inf\")\npatience = 1\npatience_counter = 0\n\nfor epoch in range(num_epochs):\n    # -----------------------\n    # TRAINING\n    # -----------------------\n    model.train()\n    total_train_loss = 0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch} training\"):\n        input_ids = batch[\"input_ids\"].to(model.device)\n\n        outputs = model(input_ids, labels=input_ids)\n        loss = outputs.loss\n        total_train_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n\n    # -----------------------\n    # VALIDATION\n    # -----------------------\n    model.eval()\n    total_val_loss = 0\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Epoch {epoch} validation\"):\n            input_ids = batch[\"input_ids\"].to(model.device)\n\n            outputs = model(input_ids, labels=input_ids)\n            total_val_loss += outputs.loss.item()\n\n    avg_val_loss = total_val_loss / len(val_loader)\n\n    print(f\"\\nEpoch {epoch}:\")\n    print(f\"  Train loss: {avg_train_loss:.4f}\")\n    print(f\"  Val loss: {avg_val_loss:.4f}\")\n\n    # -----------------------\n    # EARLY STOPPING\n    # -----------------------\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n    else:\n        patience_counter += 1\n\n        if patience_counter >= patience:\n            break\n\nprint(\"Training complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm import tqdm\n\ndef compute_ppl(model, loader, device=\"cuda\"):\n    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n\n    total_loss = 0\n    total_tokens = 0\n\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            ids = batch[\"input_ids\"].to(device)\n\n            out = model(ids, use_cache=False)\n            logits = out.logits\n\n            shift_logits = logits[:, :-1, :].contiguous()\n            shift_labels = ids[:, 1:].contiguous()\n\n            loss = loss_fn(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1)\n            )\n\n            total_loss += loss.sum().item()\n            total_tokens += loss.numel()\n\n    mean_nll = total_loss / total_tokens\n    return float(np.exp(mean_nll))\n\nppl = compute_ppl(model, val_loader)\nprint(\"PPL:\", ppl)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TweetEval","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load TweetEval sentiment\nraw = load_dataset(\"tweet_eval\", \"sentiment\")\n\n# Map numeric labels to strings\nlabel_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n\ndef build_text(example):\n    return f\"Tweet: {example['text']}\\nSentiment: {label_map[example['label']]}\"\n\ntrain_raw = raw[\"train\"].select(range(5000))\nval_raw   = raw[\"validation\"]\n\ntrain_texts = [build_text(ex) for ex in train_raw]\nval_texts   = [build_text(ex) for ex in val_raw]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.padding_side = \"left\"\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize(batch):\n    return tokenizer(\n        batch,\n        truncation=True,\n        padding=True,\n        max_length=128\n    )\n\ntrain_tokens = tokenize(train_texts)\nval_tokens   = tokenize(val_texts)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nclass TD(torch.utils.data.Dataset):\n    def __init__(self, toks):\n        self.input_ids = toks[\"input_ids\"]\n        self.attn = toks[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n            \"attention_mask\": torch.tensor(self.attn[idx], dtype=torch.long),\n        }\n\ntrain_ds = TD(train_tokens)\nval_ds   = TD(val_tokens)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\nval_loader   = DataLoader(val_ds, batch_size=4, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\nnum_epochs = 10\n\nbest_val_loss = float(\"inf\")\npatience = 1\npatience_counter = 0\n\nfor epoch in range(num_epochs):\n    # -----------------------\n    # TRAINING\n    # -----------------------\n    model.train()\n    total_train_loss = 0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch} training\"):\n        input_ids = batch[\"input_ids\"].to(model.device)\n        attn      = batch[\"attention_mask\"].to(model.device)\n\n        out = model(input_ids, attention_mask=attn, labels=input_ids)\n        loss = out.loss\n\n        total_train_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n\n    # -----------------------\n    # VALIDATION\n    # -----------------------\n    model.eval()\n    total_val_loss = 0\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Epoch {epoch} validation\"):\n            input_ids = batch[\"input_ids\"].to(model.device)\n            attn      = batch[\"attention_mask\"].to(model.device)\n\n            out = model(input_ids, attention_mask=attn, labels=input_ids)\n            total_val_loss += out.loss.item()\n\n    avg_val_loss = total_val_loss / len(val_loader)\n\n    print(f\"\\nEpoch {epoch}: Train={avg_train_loss:.4f}  Val={avg_val_loss:.4f}\")\n\n    # -----------------------\n    # EARLY STOPPING\n    # -----------------------\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"tweeteval-best.pth\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            break\n\nprint(\"Training complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nlabel_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n\ndef build_infer_prompt(text):\n    return f\"Tweet: {text}\\nSentiment:\"\n\ndef extract_sentiment_from_text(txt):\n    txt = txt.lower()\n    if \"positive\" in txt:\n        return 2\n    if \"negative\" in txt:\n        return 0\n    if \"neutral\" in txt:\n        return 1\n    return -1   # unknown / bad generation\n\n\nmodel.eval()\n\npreds = []\nlabels = []\n\nfor ex in tqdm(raw[\"test\"]):\n    prompt = build_infer_prompt(ex[\"text\"])\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        gen = model.generate(\n            **inputs,\n            max_new_tokens=5,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n    # decode only the generated part\n    gen_text = tokenizer.decode(gen[0][inputs[\"input_ids\"].size(1):])\n\n    pred_label = extract_sentiment_from_text(gen_text)\n    true_label = ex[\"label\"]\n\n    if pred_label == -1:\n        # optional: skip unparseable outputs\n        continue\n\n    preds.append(pred_label)\n    labels.append(true_label)\n\nacc = accuracy_score(labels, preds)\nprint(\"Generative accuracy:\", acc)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}